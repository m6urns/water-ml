{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53f9459",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Model Implementation for Binary Classification\n",
    "\n",
    "This notebook implements a deep learning model for binary classification using a small numerical dataset. The model architecture will consist of dense layers with dropout and regularization to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "f02848f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install tensorflow\n",
    "# Necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l1_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "856f10a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Scheme</th>\n",
       "      <th>Taxa A1</th>\n",
       "      <th>Taxa A2</th>\n",
       "      <th>Taxa A3</th>\n",
       "      <th>Taxa A4</th>\n",
       "      <th>Taxa A5</th>\n",
       "      <th>Taxa B1</th>\n",
       "      <th>Taxa B2</th>\n",
       "      <th>Taxa B3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SA PT 1.5</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SA-1 CT</td>\n",
       "      <td>Failure</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SA-2</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SA-2 CT</td>\n",
       "      <td>Failure</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SA-3</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SA-4</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SA-5 CT</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WA-1A</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WA-1B</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WA-2</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample   Scheme Taxa A1 Taxa A2 Taxa A3 Taxa A4 Taxa A5 Taxa B1 Taxa B2  \\\n",
       "0  SA PT 1.5   Stable       0     NaN       0     NaN       0     NaN     NaN   \n",
       "1    SA-1 CT  Failure  0.0121     NaN       0     NaN  0.0001     NaN     NaN   \n",
       "2       SA-2   Stable  0.0003     NaN       0     NaN       0     NaN     NaN   \n",
       "3    SA-2 CT  Failure  0.0284     NaN       0     NaN       0     NaN     NaN   \n",
       "4       SA-3   Stable  0.0002     NaN       0     NaN       0     NaN     NaN   \n",
       "5       SA-4   Stable       0     NaN       0     NaN       0     NaN     NaN   \n",
       "6    SA-5 CT   Stable       0     NaN       0     NaN       0     NaN     NaN   \n",
       "7      WA-1A   Stable       0     NaN       0     NaN       0     NaN     NaN   \n",
       "8      WA-1B   Stable  0.0008     NaN       0     NaN       0     NaN     NaN   \n",
       "9       WA-2   Stable  0.0002     NaN       0     NaN       0     NaN     NaN   \n",
       "\n",
       "  Taxa B3  \n",
       "0       0  \n",
       "1   0.002  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "5       0  \n",
       "6  0.0001  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  "
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "data = pd.read_csv('/home/matt/Projects/water-ml/datasets/labeled.csv')\n",
    "data.replace('ND', 0, inplace=True)\n",
    "\n",
    "data_psudolabeled = pd.read_csv('/home/matt/Projects/water-ml/datasets/sheet_3_psudo_labeled.csv')\n",
    "stable_fail_labels = data_psudolabeled['Scheme'].map({1: 'Stable', 0: 'Failure'})\n",
    "data_psudolabeled['Scheme'] = stable_fail_labels\n",
    "data_psudolabeled.replace('ND', 0, inplace=True)\n",
    "\n",
    "data = pd.concat([data, data_psudolabeled], ignore_index=True)\n",
    "\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "d7d19291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Scheme</th>\n",
       "      <th>Taxa A1</th>\n",
       "      <th>Taxa A2</th>\n",
       "      <th>Taxa A3</th>\n",
       "      <th>Taxa A4</th>\n",
       "      <th>Taxa A5</th>\n",
       "      <th>Taxa B1</th>\n",
       "      <th>Taxa B2</th>\n",
       "      <th>Taxa B3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.0595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.5681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Stable</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.4335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Failure</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arkansas and Lower Mississippi</td>\n",
       "      <td>Failure</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Sample   Scheme Taxa A1 Taxa A2 Taxa A3 Taxa A4  \\\n",
       "0  Arkansas and Lower Mississippi   Stable       0       0       0       0   \n",
       "1  Arkansas and Lower Mississippi   Stable       0       0       0       0   \n",
       "2  Arkansas and Lower Mississippi   Stable       0       0       0  0.0021   \n",
       "3  Arkansas and Lower Mississippi   Stable       0  0.0008       0   0.003   \n",
       "4  Arkansas and Lower Mississippi   Stable       0  0.0077       0       0   \n",
       "5  Arkansas and Lower Mississippi   Stable       0  0.0018       0  0.0018   \n",
       "6  Arkansas and Lower Mississippi   Stable       0  0.0052       0  0.0009   \n",
       "7  Arkansas and Lower Mississippi   Stable       0  0.0032       0  0.0011   \n",
       "8  Arkansas and Lower Mississippi  Failure  0.0011  0.1171       0       0   \n",
       "9  Arkansas and Lower Mississippi  Failure  0.0011  0.0995       0  0.0011   \n",
       "\n",
       "  Taxa A5 Taxa B1 Taxa B2 Taxa B3  \n",
       "0       0       0       0  0.0056  \n",
       "1       0       0       0  0.0095  \n",
       "2       0  0.0031       0  0.5849  \n",
       "3       0       0  0.0008   0.003  \n",
       "4       0       0  0.0346  0.2308  \n",
       "5       0       0   0.014  0.0595  \n",
       "6       0  0.0061  0.0506  0.5681  \n",
       "7       0  0.0032  0.0485  0.4335  \n",
       "8       0       0       0  0.7341  \n",
       "9       0       0       0   0.569  "
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding categorical features\n",
    "categorical_columns = ['Taxa A1', 'Taxa A2', 'Taxa A3', 'Taxa A4', 'Taxa A5', 'Taxa B1', 'Taxa B2', 'Taxa B3']  # Update as needed\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype(str)\n",
    "    \n",
    "encoder = OneHotEncoder(sparse=False)    \n",
    "data_encoded = pd.DataFrame(encoder.fit_transform(data[categorical_columns]))\n",
    "data_encoded.columns = encoder.get_feature_names_out(categorical_columns)\n",
    "data.drop(categorical_columns ,axis=1, inplace=True)\n",
    "data = pd.concat([data, data_encoded], axis=1)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = data.drop(['Scheme', 'Sample'], axis=1)  # Update target and identifier columns as needed\n",
    "y = data['Scheme'].map({'Stable': 0, 'Failure': 1})\n",
    "\n",
    "# Normalizing the dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "dd8298e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = pd.read_csv('/home/matt/Projects/water-ml/datasets/unlabeled.csv')  # Update with actual path\n",
    "unlabeled_data.replace('ND', 0, inplace=True)\n",
    "\n",
    "categorical_columns = ['Taxa A1', 'Taxa A2', 'Taxa A3', 'Taxa A4', 'Taxa A5', 'Taxa B1', 'Taxa B2', 'Taxa B3']\n",
    "for col in categorical_columns:\n",
    "    unlabeled_data[col] = unlabeled_data[col].astype(str)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)    \n",
    "data_encoded = pd.DataFrame(encoder.fit_transform(unlabeled_data[categorical_columns]))\n",
    "data_encoded.columns = encoder.get_feature_names_out(categorical_columns)\n",
    "unlabeled_data.drop(categorical_columns ,axis=1, inplace=True)\n",
    "unlabeled_data = pd.concat([unlabeled_data, data_encoded], axis=1)\n",
    "\n",
    "X_unlabeled = unlabeled_data.drop(['Scheme', 'Sample'], axis=1)  # Update target and identifier columns as needed\n",
    "y_unlabeled = unlabeled_data['Scheme'].map({'Stable': 0, 'Failure': 1})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_unlabeled_scaled= scaler.fit_transform(X_unlabeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "dcc1ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Augmenting the dataset with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# TODO: Apply SMOTE after psuedo labeling\n",
    "# Balance the unlabeled data\n",
    "# X_unlabeled_smote, y_unlabeled_smote = smote.fit_resample(X_unlabeled_scaled, y_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "cd54cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "# Current Best:  \n",
    "# Best: 0.901732 using {'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
    "\n",
    "# def create_model(l1_rate=0.01, l2_rate=0.01, learning_rate=0.01, noise_level=0.05):\n",
    "#     model = Sequential()\n",
    "#     # model.add(GaussianNoise(noise_level, input_shape=(X_smote.shape[1],)))\n",
    "#     model.add(Dense(30, activation='relu', kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n",
    "#     # model.add(Dropout(0.3))\n",
    "#     model.add(Dense(15, activation='relu', kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     optimizer = Adam(learning_rate=learning_rate)\n",
    "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# Previous: l1_rate=0.01, l2_rate=0.05, learning_rate=0.05, noise_level=0.01\n",
    "# Best: 0.911039 using {'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.001, 'noise_level': 0.1}\n",
    "# Best: 0.901299 using {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
    "def create_model(l1_rate=0.01, l2_rate=0.001, learning_rate=0.01, noise_level=0.05, dropout_rate_1=0.5, dropout_rate_2=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(GaussianNoise(noise_level, input_shape=(X_smote.shape[1],)))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n",
    "    model.add(Dropout(dropout_rate_1))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n",
    "    model.add(Dropout(dropout_rate_2))\n",
    "    # model.add(Dense(30, activation='tanh', kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n",
    "    \n",
    "    # model.add(Dense(64, activation='relu', input_shape=(X_scaled.shape[1],), kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "62be0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(l1_rate=0.01, l2_rate=0.01, learning_rate=0.01, noise_level=0.05, \n",
    "#                  include_blur=False, blur_size=1, num_layers=2, units_per_layer=[8, 16], dropout_rate=0.3):\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Initial layer with GaussianNoise\n",
    "#     model.add(GaussianNoise(noise_level, input_shape=(X_smote.shape[1],)))\n",
    "    \n",
    "#     if include_blur:\n",
    "#         # Add GaussianBlur layer if included\n",
    "#         model.add(GaussianBlur(blur_size))\n",
    "\n",
    "#     for i in range(num_layers):\n",
    "#         # Add the specified number of Dense layers\n",
    "#         model.add(Dense(units_per_layer[i], activation='relu', kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "    \n",
    "#     # Output layer\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     optimizer = Adam(learning_rate=learning_rate)\n",
    "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# TODO: Just psudeolabel the unlabaled data with rnb+ and rnb models. See if this achieves better results than just the rnb and rnb+ models. Even failure would highlight the \n",
    "# need to publish more labeled sets for training. How much psudo labeling is too much? Utilizing traditional ML  models for psudeolabeling traiing data for deep learning models. \n",
    "# TODO: Look into contrastive learning for semi-supervised learning of unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "d3c3e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Assuming X_smote and y_smote are your features and labels\n",
    "\n",
    "# # Wrap the model with KerasClassifier\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'l1_rate': [0.01, 0.001],\n",
    "#     'l2_rate': [0.01, 0.001],\n",
    "#     'learning_rate': [0.01, 0.001],\n",
    "#     'noise_level': [0.01, 0.05],\n",
    "#     'include_blur': [True, False],\n",
    "#     'blur_size': [1, 2],\n",
    "#     'num_layers': [1, 2, 3],  # Number of layers\n",
    "#     'units_per_layer': [[8], [8, 16], [8, 16, 32]],  # Corresponding units in each layer\n",
    "#     'dropout_rate': [0.3, 0.5],\n",
    "#     'batch_size': [32, 64],  # Example of how to include batch size in the grid\n",
    "#     'epochs': [10, 20]  # Example of how to include epochs in the grid\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "\n",
    "# # Fit the model\n",
    "# grid_result = grid.fit(X_smote, y_smote)\n",
    "\n",
    "# # Summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "a5b17992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5946/1134884955.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'l1_rate': [0.001, 0.01, 0.1],\n",
    "    'l2_rate': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': [0.00001, 0.0001, 0.01], \n",
    "    'noise_level': [0.01, 0.05, 0.1], \n",
    "    'dropout_rate_1': [0.3, 0.5],\n",
    "    'dropout_rate_2': [0.3, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "51cc3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Implementing Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Model training with Early Stopping\n",
    "# history = model.fit(X_smote, y_smote, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "# history = model.fit(X_smote, y_smote, epochs=100, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "90586ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 15:10:31.114041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.126762: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.143552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.174599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.174611: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.192871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.304707: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.368112: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.407968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.411783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.447544: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:31.521807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 15:10:33.221231: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.302552: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.316368: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.378493: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.463761: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.530507: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.677113: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.746441: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.752167: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.758126: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.779017: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-02-22 15:10:33.850873: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa224c9ef20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd274c9ef20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff0f5ec6f20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3840ca6f20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1a7ca7af20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[581], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m      3\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_smote\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10, scoring='accuracy')\n",
    "grid_result = grid.fit(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.900000 using {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.647826 (0.146090) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.660870 (0.192290) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.652174 (0.104710) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.895652 (0.073271) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.878261 (0.088679) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.873913 (0.096145) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.865217 (0.096145) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.882609 (0.089210) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.878261 (0.086521) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.626087 (0.231866) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.643478 (0.146542) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.552174 (0.197384) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.900000 (0.064635) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.895652 (0.091614) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.882609 (0.084866) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.865217 (0.098091) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.891304 (0.089633) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.878261 (0.082035) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.556522 (0.183229) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.595652 (0.162739) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.626087 (0.144463) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.865217 (0.083519) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.856522 (0.095355) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.865217 (0.085753) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.847826 (0.101501) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.826087 (0.119861) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.869565 (0.084755) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.613043 (0.185943) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.621739 (0.213929) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.617391 (0.161280) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.839130 (0.101128) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.852174 (0.099526) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.852174 (0.091614) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.878261 (0.074803) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.856522 (0.095355) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.847826 (0.091717) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.608696 (0.181362) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.678261 (0.169732) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.556522 (0.121116) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.839130 (0.101128) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.834783 (0.094858) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.847826 (0.095751) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.873913 (0.092129) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.869565 (0.084755) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.895652 (0.085200) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.600000 (0.221526) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.647826 (0.148655) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.613043 (0.148655) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.791304 (0.072232) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.800000 (0.095652) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.795652 (0.084866) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.860870 (0.104348) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.821739 (0.126685) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.882609 (0.070241) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.547826 (0.148337) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.460870 (0.147057) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.582609 (0.188718) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.460870 (0.201319) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.408696 (0.159394) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.482609 (0.176556) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.321739 (0.217565) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.286957 (0.183641) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.495652 (0.106854) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.578261 (0.171780) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.526087 (0.117391) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.565217 (0.149353) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.565217 (0.108260) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.608696 (0.145506) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.878261 (0.106144) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.869565 (0.097220) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.869565 (0.075307) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.856522 (0.091304) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.865217 (0.118991) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.865217 (0.085753) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.660870 (0.130145) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.569565 (0.129635) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.547826 (0.149606) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.882609 (0.097318) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.865217 (0.110763) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.882609 (0.084866) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.856522 (0.108347) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.860870 (0.090785) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.843478 (0.093655) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.652174 (0.095256) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.652174 (0.201131) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.591304 (0.162913) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.856522 (0.089210) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.856522 (0.084866) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.856522 (0.093352) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.826087 (0.122975) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.821739 (0.120569) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.865217 (0.100000) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.626087 (0.185688) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.552174 (0.151925) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.569565 (0.078862) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.847826 (0.089633) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.847826 (0.095751) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.834783 (0.107910) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.891304 (0.073400) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.878261 (0.094858) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.860870 (0.098764) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.621739 (0.126087) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.660870 (0.156522) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.604348 (0.160870) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.843478 (0.101408) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.847826 (0.099621) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.839130 (0.106588) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.873913 (0.087929) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.865217 (0.117391) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.847826 (0.089633) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.634783 (0.167490) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.595652 (0.171780) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.539130 (0.219296) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.778261 (0.112457) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.808696 (0.121739) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.800000 (0.095652) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.839130 (0.102980) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.865217 (0.103712) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.847826 (0.065217) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.495652 (0.140483) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.482609 (0.128169) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.452174 (0.168615) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.308696 (0.206190) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.539130 (0.130724) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.452174 (0.170842) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.569565 (0.149921) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.295652 (0.193271) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.465217 (0.154394) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.491304 (0.181414) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.491304 (0.171780) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.339130 (0.230722) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.3, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.647826 (0.131085) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.560870 (0.126685) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.630435 (0.162971) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.873913 (0.087929) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.891304 (0.068054) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.860870 (0.100659) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.886957 (0.075807) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.856522 (0.099241) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.847826 (0.115443) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.586957 (0.164127) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.569565 (0.228788) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.643478 (0.180107) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.886957 (0.087390) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.860870 (0.086521) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.860870 (0.104348) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.873913 (0.081224) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.865217 (0.085753) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.869565 (0.126012) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.591304 (0.160576) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.560870 (0.175482) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.621739 (0.104800) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.860870 (0.090785) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.873913 (0.092129) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.847826 (0.095751) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.804348 (0.135063) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.808696 (0.139130) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.869565 (0.084755) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.556522 (0.187310) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.591304 (0.187713) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.543478 (0.132234) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.856522 (0.095355) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.847826 (0.099621) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.852174 (0.103255) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.865217 (0.087929) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.869565 (0.084755) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.882609 (0.082609) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.630435 (0.199479) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.643478 (0.128684) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.634783 (0.127799) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.847826 (0.099621) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.847826 (0.097705) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.839130 (0.099241) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.869565 (0.099146) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.886957 (0.078261) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.882609 (0.082609) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.578261 (0.101128) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.573913 (0.175859) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.547826 (0.127799) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.769565 (0.087065) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.765217 (0.082951) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.760870 (0.105160) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.860870 (0.079697) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.860870 (0.063900) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.878261 (0.090785) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.552174 (0.111782) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.395652 (0.149921) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.513043 (0.195217) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.578261 (0.201178) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.473913 (0.158502) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.447826 (0.167321) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.300000 (0.197766) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.491304 (0.192536) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.469565 (0.163608) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.586957 (0.184719) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.286957 (0.183641) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.586957 (0.195652) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.560870 (0.140819) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.604348 (0.139470) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.886957 (0.099526) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.878261 (0.096831) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.826087 (0.121428) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.860870 (0.111359) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.869565 (0.095256) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.843478 (0.121739) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.573913 (0.150362) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.508696 (0.160399) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.613043 (0.156098) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.852174 (0.116988) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.895652 (0.087390) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.860870 (0.104348) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.852174 (0.112036) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.856522 (0.102980) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.891304 (0.087498) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.613043 (0.129635) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.565217 (0.126012) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.565217 (0.149353) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.856522 (0.095355) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.869565 (0.084755) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.869565 (0.084755) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.860870 (0.074803) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.786957 (0.167772) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.873913 (0.103712) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.508696 (0.158024) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.543478 (0.211440) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.639130 (0.133373) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.865217 (0.098091) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.839130 (0.104800) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.839130 (0.101128) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.878261 (0.088679) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.865217 (0.107295) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.865217 (0.094158) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.600000 (0.162448) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.604348 (0.152422) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.582609 (0.112036) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.843478 (0.095652) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.843478 (0.105070) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.843478 (0.103255) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.856522 (0.111782) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.873913 (0.090054) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.891304 (0.048610) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.526087 (0.135342) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.547826 (0.182609) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.452174 (0.134993) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.721739 (0.121739) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.726087 (0.130507) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.739130 (0.089104) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.860870 (0.077289) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.860870 (0.100659) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.847826 (0.085311) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.01, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.573913 (0.130145) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.404348 (0.077898) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.500000 (0.136455) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.291304 (0.188567) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.343478 (0.233693) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.001, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.426087 (0.174780) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.421739 (0.171780) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.591304 (0.120176) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.291304 (0.188567) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.286957 (0.183641) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.1}\n",
      "0.482609 (0.228788) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.01}\n",
      "0.495652 (0.155795) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.05}\n",
      "0.486957 (0.157725) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 1e-05, 'noise_level': 0.1}\n",
      "0.334783 (0.227629) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.01}\n",
      "0.282609 (0.178473) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.05}\n",
      "0.265217 (0.154883) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.0001, 'noise_level': 0.1}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.01}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
      "0.260870 (0.148082) with: {'dropout_rate_1': 0.5, 'dropout_rate_2': 0.5, 'l1_rate': 0.1, 'l2_rate': 0.1, 'learning_rate': 0.01, 'noise_level': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Current Best:  Switch to matthews_corrcoef for grid search scoring\n",
    "# Best: 0.901732 using {'l1_rate': 0.01, 'l2_rate': 0.01, 'learning_rate': 0.01, 'noise_level': 0.05}\n",
    "# Best: 0.911039 using {'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.001, 'noise_level': 0.1}\n",
    "\n",
    "# Previous Best:\n",
    "# 0.882428 (0.052831) with: {'l1_rate': 0.001, 'l2_rate': 0.1, 'learning_rate': 0.001} CV:3 noise_level=0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
